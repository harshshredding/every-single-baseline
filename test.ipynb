{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad3fc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n",
      "DROPBOX: connection verified\n"
     ]
    }
   ],
   "source": [
    "from utils.easy_testing import get_bert_tokenizer, get_train_samples_by_dataset_name, get_test_samples_by_dataset_name, get_valid_samples_by_dataset_name\n",
    "tokenizer = get_bert_tokenizer('xlm-roberta-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8648d7",
   "metadata": {},
   "source": [
    "# Compare Old and New Seq Labeling approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefdd2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n",
      "DROPBOX: connection verified\n"
     ]
    }
   ],
   "source": [
    "import train_util\n",
    "from utils.easy_testing import get_dataset_config_by_name\n",
    "valid_samples_new = train_util.get_valid_samples(get_dataset_config_by_name('multiconer_fine_vanilla'))\n",
    "valid_samples_old = train_util.get_valid_samples(get_dataset_config_by_name('multiconer_fine_tokens'))\n",
    "first_sample_new = valid_samples_new[0]\n",
    "first_sample_old = valid_samples_old[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9be7e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import get_experiment_config\n",
    "\n",
    "seq_new_experiment = get_experiment_config(\n",
    "    model_config_module_name='model_seq_base_no_special_tokens',\n",
    "    dataset_config_name='multiconer_fine_vanilla'\n",
    ")\n",
    "\n",
    "span_old_experiment = get_experiment_config(\n",
    "    model_config_module_name='model_span_large_custom_tokenization_no_batch',\n",
    "        dataset_config_name='multiconer_fine_tokens'\n",
    "    ),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ae733a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from train_util import prepare_model\n",
    "new_seq_model = prepare_model(seq_new_experiment.model_config, seq_new_experiment.dataset_config)\n",
    "old_seq_model = prepare_model(seq_old_experiment.model_config, seq_old_experiment.dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f99e13e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'all_models.SeqLabelerNoTokenization'>\n",
      "<class 'all_models.JustBert3Classes'>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Gold bio labels dont match:\nnew[o, o, Athlete-BEGIN, Athlete-BEGIN, o, OtherPER-BEGIN, OtherPER-INSIDE, o, OtherPER-BEGIN, OtherPER-INSIDE, OtherPER-INSIDE, OtherPER-INSIDE, o, o, o, o, o, o]\nold[o, o, Athlete-BEGIN, Athlete-INSIDE, o, OtherPER-BEGIN, OtherPER-INSIDE, o, OtherPER-BEGIN, OtherPER-INSIDE, OtherPER-INSIDE, OtherPER-INSIDE, o, o, o, o, o, o]\nGold Annos New:[Anno(begin_offset=7, end_offset=12, label_type='Athlete', extraction='rossi', features={}), Anno(begin_offset=15, end_offset=21, label_type='OtherPER', extraction='biaggi', features={}), Anno(begin_offset=24, end_offset=37, label_type='OtherPER', extraction='sete gibernau', features={})]\nGOld Annos Old:[Anno(begin_offset=7, end_offset=12, label_type='Athlete', extraction='rossi', features={}), Anno(begin_offset=15, end_offset=21, label_type='OtherPER', extraction='biaggi', features={}), Anno(begin_offset=24, end_offset=37, label_type='OtherPER', extraction='sete gibernau', features={})]\nsample_id:bb478bfb-3271-49eb-80b6-6cf4797a89c5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m loss_old, predictions_old \u001b[38;5;241m=\u001b[39m old_seq_model([valid_sample_old], collect_old)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m((collect_new[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minput_ids \u001b[38;5;241m==\u001b[39m collect_old[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minput_ids)\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]),\\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput ids dont match:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mnew\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollect_new[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollect_old[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m collect_new[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m collect_old[\u001b[38;5;241m1\u001b[39m],\\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGold bio labels dont match:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mnew\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollect_new[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollect_old[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGold Annos New:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_sample_new\u001b[38;5;241m.\u001b[39mannos\u001b[38;5;241m.\u001b[39mgold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGOld Annos Old:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_sample_old\u001b[38;5;241m.\u001b[39mannos\u001b[38;5;241m.\u001b[39mgold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124msample_id:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_sample_new\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Gold bio labels dont match:\nnew[o, o, Athlete-BEGIN, Athlete-BEGIN, o, OtherPER-BEGIN, OtherPER-INSIDE, o, OtherPER-BEGIN, OtherPER-INSIDE, OtherPER-INSIDE, OtherPER-INSIDE, o, o, o, o, o, o]\nold[o, o, Athlete-BEGIN, Athlete-INSIDE, o, OtherPER-BEGIN, OtherPER-INSIDE, o, OtherPER-BEGIN, OtherPER-INSIDE, OtherPER-INSIDE, OtherPER-INSIDE, o, o, o, o, o, o]\nGold Annos New:[Anno(begin_offset=7, end_offset=12, label_type='Athlete', extraction='rossi', features={}), Anno(begin_offset=15, end_offset=21, label_type='OtherPER', extraction='biaggi', features={}), Anno(begin_offset=24, end_offset=37, label_type='OtherPER', extraction='sete gibernau', features={})]\nGOld Annos Old:[Anno(begin_offset=7, end_offset=12, label_type='Athlete', extraction='rossi', features={}), Anno(begin_offset=15, end_offset=21, label_type='OtherPER', extraction='biaggi', features={}), Anno(begin_offset=24, end_offset=37, label_type='OtherPER', extraction='sete gibernau', features={})]\nsample_id:bb478bfb-3271-49eb-80b6-6cf4797a89c5"
     ]
    }
   ],
   "source": [
    "print(type(new_seq_model))\n",
    "print(type(old_seq_model))\n",
    "for valid_sample_new, valid_sample_old in zip(valid_samples_new, valid_samples_old):\n",
    "    collect_new = []\n",
    "    collect_old = []\n",
    "    loss_new, predictions_new = new_seq_model([valid_sample_new], collect_new)\n",
    "    loss_old, predictions_old = old_seq_model([valid_sample_old], collect_old)\n",
    "    assert all((collect_new[0].input_ids == collect_old[0].input_ids).numpy()[0]),\\\n",
    "        f\"Input ids dont match:\\nnew{collect_new[0].input_ids}\\nold{collect_old[0].input_ids}\"\n",
    "    assert collect_new[1] == collect_old[1],\\\n",
    "        f\"Gold bio labels dont match:\\nnew{collect_new[1]}\\nold{collect_old[1]}\"\\\n",
    "        f\"\\nGold Annos New:{valid_sample_new.annos.gold}\"\\\n",
    "        f\"\\nGOld Annos Old:{valid_sample_old.annos.gold}\"\\\n",
    "        f\"\\nsample_id:{valid_sample_new.id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62827300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all((collect_new[0].input_ids == collect_old[0].input_ids).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "359ba6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_new[1] == collect_old[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8b5d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample = valid_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e85a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "chatgpt_predictions_file_path = './chatgpt_social_dis_ner_test.json'\n",
    "with open(chatgpt_predictions_file_path, 'r') as chat_gpt_predictions_file:\n",
    "    chat_gpt_predictions = json.load(chat_gpt_predictions_file)\n",
    "\n",
    "chat_gpt_response_dict = {\n",
    "    sample_id: diseases\n",
    "    for sample_id, diseases in chat_gpt_predictions\n",
    "}\n",
    "\n",
    "    \n",
    "chat_gpt_tsv_file = '/home/harsh/Documents/social_dis_ner_submisssions/experiment_chatgpt_social_dis_ner_social_dis_ner_chatgpt_model_seq_large_default_test_epoch_2_predictions.tsv'\n",
    "df = pd.read_csv(chat_gpt_tsv_file, sep='\\t')\n",
    "\n",
    "with open('final_seq.tsv', 'w') as output_tsv: \n",
    "    writer = csv.writer(output_tsv, delimiter='\\t', lineterminator='\\n')\n",
    "    writer.writerow(['tweets_id', 'begin', 'end', 'type', 'extraction'])\n",
    "    for _, row in df.iterrows():\n",
    "        sample_id = str(row['sample_id'])\n",
    "        assert sample_id in chat_gpt_response_dict, f\"{row['sample_id']}\"\n",
    "        chat_gpt_response = chat_gpt_response_dict[sample_id]\n",
    "        gpt_predictions_prefix = chat_gpt_response + ' [SEP] '\n",
    "        begin = int(row['begin'])\n",
    "        end = int(row['end'])\n",
    "        begin -= len(gpt_predictions_prefix)\n",
    "        end -= len(gpt_predictions_prefix)\n",
    "        writer.writerow([sample_id, str(begin), str(end), row['type'], row['extraction']]) \n",
    "     \n",
    "\n",
    "# with open('/home/harsh/Documents/social_dis_ner_submisssions/trimmed.tsv', 'w') as trimmed:\n",
    "#     with open('/home/harsh/Documents/social_dis_ner_submisssions/experiment_chatgpt_social_dis_ner_social_dis_ner_chatgpt_span_large_default_test_epoch_7_predictions.tsv', 'r') as predictions_file:\n",
    "#         for line in predictions_file:\n",
    "#             line = line.strip()\n",
    "#             print(line, file=trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "036caad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n",
      "DROPBOX: connection verified\n"
     ]
    }
   ],
   "source": [
    "import train_util\n",
    "from utils.easy_testing import get_dataset_config_by_name\n",
    "\n",
    "test_samples = train_util.get_test_samples(get_dataset_config_by_name('social_dis_ner_vanilla'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "890eb432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample(text='Siiiiiiiii siiiiiii siiiiioooiiiii joder siiiiiiiii\\n@vicentsempere confío en ti\\nYesssssssss yesssss\\nFeliz felizzzzz\\n\\n\\n\\n\\n\\n', id='1132390610068021248', annos=AnnotationCollection(gold=[], external=[]))\n"
     ]
    }
   ],
   "source": [
    "print(test_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a559115a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(model_config_name='span_large_default', pretrained_model_name='xlm-roberta-large', pretrained_model_output_dim=1024, num_epochs=15, model_name='SpanNoTokenizationBatched', optimizer='Adam', learning_rate=1e-05, batch_size=4, max_span_length=None, use_special_bert_tokens=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.config import get_model_config_from_module\n",
    "get_model_config_from_module('model_span_large_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dbdd951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pancreatitis, Gastritis crónica, Cirrosis, Hepatitis, Fatiga Crónica, Cáncer de garganta, Mal aliento, Cardiomiopatía alcohólica'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.openai import chatgpt_get_diseases_in_tweet\n",
    "\n",
    "tweet = \"\"\"\n",
    "Consecuencias del #Alcoholismo\n",
    "  #Pancreatitis\n",
    "  Gastritis crónica\n",
    "  Cirrosis\n",
    " Hepatitis\n",
    "  Fatiga Crónica\n",
    "  Desconcentración\n",
    "  Falta de memoria\n",
    "  Cáncer de garganta\n",
    "  Mal aliento\n",
    "  Cardiomiopatía alcohólica\n",
    "#DiaMundialsinalcohol          https://t.co/LGHq6yjixb\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chatgpt_get_diseases_in_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "608ade1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['1440492653229993984', 'lupus'], ['1440492653229993984', 'lupus']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "with open(\"chatgpt_social_dis_ner_valid.json\", 'r') as gpt_predictions_file:\n",
    "    chat_gpt_predictions = json.load(gpt_predictions_file)\n",
    "\n",
    "print(len(chat_gpt_predictions))\n",
    "sample_ids = [sample_id for sample_id, _ in chat_gpt_predictions]\n",
    "sample_id_counts = Counter(sample_ids)\n",
    "\n",
    "[prediction for prediction in chat_gpt_predictions if prediction[0] == '1440492653229993984']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4749ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53ba5cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'diabetes, cardiopatías, obesidad.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f757fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35378, 903, 83, 1950, 1495, 5]\n",
      "[35378, 903, 83, 1950, 1495, 6, 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_tokenization_differences(tokenizer, sample_text, tokens) -> None | tuple: \n",
    "    encoding_ids = tokenizer(sample_text, add_special_tokens=False).input_ids\n",
    "    encoding_split_ids = tokenizer(tokens, add_special_tokens=False, is_split_into_words=True).input_ids\n",
    "    if encoding_ids != encoding_split_ids:\n",
    "        return (encoding_ids, encoding_split_ids)        \n",
    "\n",
    "def print_difference(diff):\n",
    "    if diff is not None:\n",
    "        raw, split = diff\n",
    "        print(raw)\n",
    "        print(split)\n",
    "        print()\n",
    "    \n",
    "diff = get_tokenization_differences(tokenizer, \"Hello this is Harsh.\", [\"Hello\", \"this\", \"is\", \"Harsh\", \".\"])\n",
    "print_difference(diff)\n",
    "\n",
    "diff = get_tokenization_differences(tokenizer, \"Yo lo lololo\", [\"Yo\", \"lo\", \"lololo\"])\n",
    "print_difference(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c373de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 903, 83, 142, 27781, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"this is an example\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb58d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_tokens_from_sample\n",
    "\n",
    "train_samples = get_train_samples_by_dataset_name('multiconer_fine_tokens')\n",
    "for sample in train_samples:\n",
    "    sample_text = sample.text\n",
    "    tokens = get_tokens_from_sample(sample)\n",
    "    diff = get_tokenization_differences(tokenizer, sample_text, tokens)\n",
    "    print_difference(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bac77fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_samples = get_valid_samples_by_dataset_name('multiconer_fine_tokens')\n",
    "for sample in valid_samples:\n",
    "    sample_text = sample.text\n",
    "    tokens = get_tokens_from_sample(sample)\n",
    "    diff = get_tokenization_differences(tokenizer, sample_text, tokens)\n",
    "    print_difference(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d392e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = get_test_samples_by_dataset_name('multiconer_fine_tokens')\n",
    "for sample in test_samples:\n",
    "    sample_text = sample.text\n",
    "    tokens = get_tokens_from_sample(sample)\n",
    "    diff = get_tokenization_differences(tokenizer, sample_text, tokens)\n",
    "    print_difference(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "840606c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     /home/harsh/nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from preprocessors.multiconer_preprocessor import read_raw_data\n",
    "test_tokens = read_raw_data('multiconer2023/EN-English/en_test.conll')\n",
    "pred_tokens = read_raw_data('submission/my_submission_preds/en.pred.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "672933de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/every/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: _ O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "                    O       1.00      0.00      0.00         9\n",
      "AerospaceManufacturer       0.35      0.61      0.44      1015\n",
      "  AnatomicalStructure       0.61      0.65      0.63      5838\n",
      "              ArtWork       0.37      0.46      0.41      1270\n",
      "               Artist       0.72      0.77      0.74     57034\n",
      "              Athlete       0.74      0.72      0.73     27636\n",
      "      CarManufacturer       0.58      0.58      0.58      2984\n",
      "               Cleric       0.52      0.43      0.47      4732\n",
      "             Clothing       0.46      0.59      0.52      2244\n",
      "              Disease       0.61      0.68      0.64      5623\n",
      "                Drink       0.52      0.52      0.52      2246\n",
      "             Facility       0.60      0.60      0.60     16185\n",
      "                 Food       0.49      0.47      0.48      5317\n",
      "      HumanSettlement       0.82      0.86      0.84     41103\n",
      "     MedicalProcedure       0.56      0.59      0.58      3850\n",
      "   Medication/Vaccine       0.71      0.66      0.69      5421\n",
      "           MusicalGRP       0.67      0.52      0.59     12969\n",
      "          MusicalWork       0.68      0.62      0.65     15304\n",
      "                  ORG       0.64      0.48      0.55     22414\n",
      "             OtherLOC       0.70      0.22      0.34      4635\n",
      "             OtherPER       0.38      0.48      0.42     22028\n",
      "            OtherPROD       0.39      0.49      0.43     11838\n",
      "           Politician       0.51      0.52      0.51     15990\n",
      "          PrivateCorp       0.18      0.40      0.25       810\n",
      "           PublicCorp       0.53      0.47      0.50      6825\n",
      "            Scientist       0.52      0.26      0.35      4928\n",
      "             Software       0.64      0.63      0.63      8962\n",
      "            SportsGRP       0.75      0.75      0.75     13009\n",
      "        SportsManager       0.52      0.54      0.53      5333\n",
      "              Station       0.66      0.70      0.68      5978\n",
      "              Symptom       0.46      0.44      0.45      1759\n",
      "              Vehicle       0.41      0.44      0.42      5935\n",
      "           VisualWork       0.63      0.63      0.63     19678\n",
      "          WrittenWork       0.72      0.53      0.61     16912\n",
      "\n",
      "            micro avg       0.63      0.63      0.63    377814\n",
      "            macro avg       0.58      0.54      0.53    377814\n",
      "         weighted avg       0.64      0.63      0.63    377814\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "all_tags_gold = []\n",
    "all_tags_pred = []\n",
    "for sample_id in test_tokens:\n",
    "    assert len(test_tokens[sample_id]) == len(pred_tokens[sample_id])\n",
    "    gold_tags = [tag for _, tag in test_tokens[sample_id]]\n",
    "    pred_tags = [tag for _, tag in pred_tokens[sample_id]]\n",
    "    all_tags_gold.append(gold_tags)\n",
    "    all_tags_pred.append(pred_tags)\n",
    "assert len(all_tags_gold) == len(all_tags_pred)\n",
    "print(classification_report(all_tags_gold, all_tags_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85f1abdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-Vehicle', 'B-Symptom', 'B-MusicalWork', 'I-MusicalWork', 'I-SportsManager', 'I-OtherLOC', 'I-OtherPROD', 'B-SportsManager', 'B-AerospaceManufacturer', 'B-OtherPROD', 'B-WrittenWork', 'B-Scientist', 'B-Cleric', 'I-AerospaceManufacturer', 'B-Drink', 'B-PrivateCorp', 'B-Disease', 'B-Facility', 'B-SportsGRP', 'B-AnatomicalStructure', 'I-Software', 'I-Disease', 'I-Medication/Vaccine', 'B-Medication/Vaccine', 'B-CarManufacturer', 'B-Politician', 'B-ORG', 'I-MusicalGRP', 'I-OtherPER', 'I-SportsGRP', 'I-Politician', 'I-Athlete', 'I-Drink', 'I-Station', 'B-MusicalGRP', 'B-Food', 'I-Vehicle', 'I-Symptom', 'I-HumanSettlement', 'B-ArtWork', 'I-Food', 'I-MedicalProcedure', 'I-ORG', 'B-Clothing', 'I-Clothing', 'I-Cleric', 'B-OtherPER', 'B-MedicalProcedure', 'B-VisualWork', 'I-Facility', 'I-PublicCorp', 'I-ArtWork', 'I-AnatomicalStructure', 'B-Artist', 'I-Scientist', 'B-PublicCorp', 'B-Station', 'B-Athlete', 'B-OtherLOC', 'I-PrivateCorp', 'O', '_ O', 'B-Software', 'B-HumanSettlement', 'I-WrittenWork', 'I-VisualWork', 'I-CarManufacturer', 'I-Artist'}\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "num_errors = 0\n",
    "all_tags_set = set()\n",
    "for tags in all_tags_gold:\n",
    "    all_tags_set.update(tags)\n",
    "    num_errors += len([tag for tag in tags if tag == '_ O'])\n",
    "print(all_tags_set)\n",
    "print(num_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae21bc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = set([1,2,3,4])\n",
    "x.update([4,5,6])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40feb8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "class SomeClass(object):\n",
    "    def __init__(self, name: str, age: int):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "args = {\"name\": \"John\", \"age\": 50}\n",
    "x = SomeClass(**args)\n",
    "print(x.name)\n",
    "\n",
    "print(x.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d824b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'preprocessor_class_path': 'preprocessors.multiconer_preprocessor.PreprocessMulticoner', 'preprocessor_type': 'tokens', 'dataset': 'multiconer_fine', 'dataset_splits': ['train', 'test', 'valid']}\n"
     ]
    }
   ],
   "source": [
    "from preprocess import get_preprocessor_config\n",
    "config = get_preprocessor_config('preprocessor_config_multiconer_with_tokens')\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f17b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0afaaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from preamble import *\n",
    "from structs import Anno, Sample, AnnotationCollection\n",
    "\n",
    "def get_sample_text_from_passage(passage_soup: BeautifulSoup) -> str:\n",
    "    children_names = [child.name for child in passage_soup.children]\n",
    "    assert 'text' in children_names\n",
    "    sample_text = passage_soup.find('text').text\n",
    "    return sample_text\n",
    "\n",
    "\n",
    "def get_annotation_type(anno_soup: BeautifulSoup) -> str:\n",
    "    type_info = [info for info in anno_soup.find_all('infon') if info['key'] == 'class'][0]\n",
    "    return type_info.text\n",
    "\n",
    "\n",
    "def get_annos_from_passage(passage_soup: BeautifulSoup, passage_offset: int) -> List[Anno]:\n",
    "    ret = []\n",
    "    anno_soups = passage_soup.find_all('annotation')\n",
    "    for anno_soup in anno_soups:\n",
    "        anno_type = get_annotation_type(anno_soup)\n",
    "        anno_start = int(anno_soup.location['offset'])\n",
    "        anno_end = int(anno_soup.location['length']) + anno_start\n",
    "        anno_start -= passage_offset\n",
    "        anno_end -= passage_offset\n",
    "        anno_text = anno_soup.find('text').text\n",
    "        ret.append(\n",
    "            Anno(\n",
    "                begin_offset=anno_start,\n",
    "                end_offset=anno_end,\n",
    "                label_type=anno_type,\n",
    "                extraction=anno_text\n",
    "            )\n",
    "        )\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_samples_from_bioc_file(bioc_xml_file_path: str) -> List[Sample]:\n",
    "    with open(bioc_xml_file_path, 'r') as cdr_xml_file:\n",
    "        cdr_raw_xml_data = cdr_xml_file.read()\n",
    "    cdr_soup = BeautifulSoup(cdr_raw_xml_data, features='xml')\n",
    "    all_documents = cdr_soup.find_all('document')\n",
    "    ret = []\n",
    "    for cdr_document in all_documents:\n",
    "        cdr_document_id = cdr_document.id.text\n",
    "        for passage_idx, passage_soup in enumerate(cdr_document.find_all('passage')):\n",
    "            sample_text = get_sample_text_from_passage(passage_soup)\n",
    "            passage_offset = int(passage_soup.offset.text)\n",
    "            gold_annos = get_annos_from_passage(passage_soup, passage_offset)\n",
    "            ret.append(\n",
    "                Sample(\n",
    "                    text=sample_text,\n",
    "                    annos=AnnotationCollection(gold=gold_annos, external=[]),\n",
    "                    id=(cdr_document_id + str(passage_idx))\n",
    "                )\n",
    "            )\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24109435",
   "metadata": {},
   "outputs": [],
   "source": [
    "chem_samples = get_samples_from_bioc_file('chemdner_corpus/training.bioc.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "808af000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "Sample(text='DPP6 as a candidate gene for neuroleptic-induced tardive dyskinesia.', id='218260850', annos=AnnotationCollection(gold=[], external=[]))\n",
      "Sample(text='We implemented a two-step approach to detect potential predictor gene variants for neuroleptic-induced tardive dyskinesia (TD) in schizophrenic subjects. First, we screened associations by using a genome-wide (Illumina HumanHapCNV370) SNP array in 61 Japanese schizophrenia patients with treatment-resistant TD and 61 Japanese schizophrenia patients without TD. Next, we performed a replication analysis in 36 treatment-resistant TD and 138 non-TD subjects. An association of an SNP in the DPP6 (dipeptidyl peptidase-like protein-6) gene, rs6977820, the most promising association identified by the screen, was significant in the replication sample (allelic P=0.008 in the replication sample, allelic P=4.6 × 10(-6), odds ratio 2.32 in the combined sample). The SNP is located in intron-1 of the DPP6 gene and the risk allele was associated with decreased DPP6 gene expression in the human postmortem prefrontal cortex. Chronic administration of haloperidol increased Dpp6 expression in mouse brains. DPP6 is an auxiliary subunit of Kv4 and regulates the properties of Kv4, which regulates the activity of dopaminergic neurons. The findings of this study indicate that an altered response of Kv4/DPP6 to long-term neuroleptic administration is involved in neuroleptic-induced TD.', id='218260851', annos=AnnotationCollection(gold=[Anno(begin_offset=946, end_offset=957, label_type='TRIVIAL', extraction='haloperidol', features={})], external=[]))\n"
     ]
    }
   ],
   "source": [
    "print(len(chem_samples))\n",
    "print(chem_samples[0])\n",
    "print(chem_samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a09b18eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 500 documents\n",
      "<document>\n",
      "<id>6794356</id>\n",
      "<passage>\n",
      "<infon key=\"type\">title</infon>\n",
      "<offset>0</offset>\n",
      "<text>Tricuspid valve regurgitation and lithium carbonate toxicity in a newborn infant.</text>\n",
      "<annotation id=\"0\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D014262</infon>\n",
      "<location length=\"29\" offset=\"0\"></location>\n",
      "<text>Tricuspid valve regurgitation</text>\n",
      "</annotation>\n",
      "<annotation id=\"1\">\n",
      "<infon key=\"type\">Chemical</infon>\n",
      "<infon key=\"MESH\">D016651</infon>\n",
      "<location length=\"17\" offset=\"34\"></location>\n",
      "<text>lithium carbonate</text>\n",
      "</annotation>\n",
      "<annotation id=\"2\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D064420</infon>\n",
      "<location length=\"8\" offset=\"52\"></location>\n",
      "<text>toxicity</text>\n",
      "</annotation>\n",
      "</passage>\n",
      "<passage>\n",
      "<infon key=\"type\">abstract</infon>\n",
      "<offset>82</offset>\n",
      "<text>A newborn with massive tricuspid regurgitation, atrial flutter, congestive heart failure, and a high serum lithium level is described. This is the first patient to initially manifest tricuspid regurgitation and atrial flutter, and the 11th described patient with cardiac disease among infants exposed to lithium compounds in the first trimester of pregnancy. Sixty-three percent of these infants had tricuspid valve involvement. Lithium carbonate may be a factor in the increasing incidence of congenital heart disease when taken during early pregnancy. It also causes neurologic depression, cyanosis, and cardiac arrhythmia when consumed prior to delivery.</text>\n",
      "<annotation id=\"3\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D014262</infon>\n",
      "<location length=\"23\" offset=\"105\"></location>\n",
      "<text>tricuspid regurgitation</text>\n",
      "</annotation>\n",
      "<annotation id=\"4\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D001282</infon>\n",
      "<location length=\"14\" offset=\"130\"></location>\n",
      "<text>atrial flutter</text>\n",
      "</annotation>\n",
      "<annotation id=\"5\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D006333</infon>\n",
      "<location length=\"24\" offset=\"146\"></location>\n",
      "<text>congestive heart failure</text>\n",
      "</annotation>\n",
      "<annotation id=\"6\">\n",
      "<infon key=\"type\">Chemical</infon>\n",
      "<infon key=\"MESH\">D008094</infon>\n",
      "<location length=\"7\" offset=\"189\"></location>\n",
      "<text>lithium</text>\n",
      "</annotation>\n",
      "<annotation id=\"7\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D014262</infon>\n",
      "<location length=\"23\" offset=\"265\"></location>\n",
      "<text>tricuspid regurgitation</text>\n",
      "</annotation>\n",
      "<annotation id=\"8\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D001282</infon>\n",
      "<location length=\"14\" offset=\"293\"></location>\n",
      "<text>atrial flutter</text>\n",
      "</annotation>\n",
      "<annotation id=\"9\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D006331</infon>\n",
      "<location length=\"15\" offset=\"345\"></location>\n",
      "<text>cardiac disease</text>\n",
      "</annotation>\n",
      "<annotation id=\"10\">\n",
      "<infon key=\"type\">Chemical</infon>\n",
      "<infon key=\"MESH\">D008094</infon>\n",
      "<location length=\"7\" offset=\"386\"></location>\n",
      "<text>lithium</text>\n",
      "</annotation>\n",
      "<annotation id=\"11\">\n",
      "<infon key=\"type\">Chemical</infon>\n",
      "<infon key=\"MESH\">D016651</infon>\n",
      "<location length=\"17\" offset=\"511\"></location>\n",
      "<text>Lithium carbonate</text>\n",
      "</annotation>\n",
      "<annotation id=\"12\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D006331</infon>\n",
      "<location length=\"24\" offset=\"576\"></location>\n",
      "<text>congenital heart disease</text>\n",
      "</annotation>\n",
      "<annotation id=\"13\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D003866</infon>\n",
      "<location length=\"21\" offset=\"651\"></location>\n",
      "<text>neurologic depression</text>\n",
      "</annotation>\n",
      "<annotation id=\"14\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D003490</infon>\n",
      "<location length=\"8\" offset=\"674\"></location>\n",
      "<text>cyanosis</text>\n",
      "</annotation>\n",
      "<annotation id=\"15\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D001145</infon>\n",
      "<location length=\"18\" offset=\"688\"></location>\n",
      "<text>cardiac arrhythmia</text>\n",
      "</annotation>\n",
      "</passage>\n",
      "<relation id=\"R0\">\n",
      "<infon key=\"relation\">CID</infon>\n",
      "<infon key=\"Chemical\">D016651</infon>\n",
      "<infon key=\"Disease\">D003490</infon>\n",
      "</relation>\n",
      "<relation id=\"R1\">\n",
      "<infon key=\"relation\">CID</infon>\n",
      "<infon key=\"Chemical\">D016651</infon>\n",
      "<infon key=\"Disease\">D001145</infon>\n",
      "</relation>\n",
      "<relation id=\"R2\">\n",
      "<infon key=\"relation\">CID</infon>\n",
      "<infon key=\"Chemical\">D016651</infon>\n",
      "<infon key=\"Disease\">D003866</infon>\n",
      "</relation>\n",
      "</document>\n"
     ]
    }
   ],
   "source": [
    "cdr_documents_soup = cdr_soup.find_all('document')\n",
    "print(f\"found {len(cdr_documents_soup)} documents\")\n",
    "first_document = cdr_documents_soup[0]\n",
    "print(first_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2d09638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "print(type(first_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b400d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<passage>\n",
      "<infon key=\"type\">title</infon>\n",
      "<offset>0</offset>\n",
      "<text>Tricuspid valve regurgitation and lithium carbonate toxicity in a newborn infant.</text>\n",
      "<annotation id=\"0\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D014262</infon>\n",
      "<location length=\"29\" offset=\"0\"></location>\n",
      "<text>Tricuspid valve regurgitation</text>\n",
      "</annotation>\n",
      "<annotation id=\"1\">\n",
      "<infon key=\"type\">Chemical</infon>\n",
      "<infon key=\"MESH\">D016651</infon>\n",
      "<location length=\"17\" offset=\"34\"></location>\n",
      "<text>lithium carbonate</text>\n",
      "</annotation>\n",
      "<annotation id=\"2\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D064420</infon>\n",
      "<location length=\"8\" offset=\"52\"></location>\n",
      "<text>toxicity</text>\n",
      "</annotation>\n",
      "</passage>\n",
      "--------------------\n",
      "A newborn with massive tricuspid regurgitation, atrial flutter, congestive heart failure, and a high serum lithium level is described. This is the first patient to initially manifest tricuspid regurgitation and atrial flutter, and the 11th described patient with cardiac disease among infants exposed to lithium compounds in the first trimester of pregnancy. Sixty-three percent of these infants had tricuspid valve involvement. Lithium carbonate may be a factor in the increasing incidence of congenital heart disease when taken during early pregnancy. It also causes neurologic depression, cyanosis, and cardiac arrhythmia when consumed prior to delivery.\n",
      "<passage>\n",
      "<infon key=\"type\">abstract</infon>\n",
      "<offset>82</offset>\n",
      "<text>A newborn with massive tricuspid regurgitation, atrial flutter, congestive heart failure, and a high serum lithium level is described. This is the first patient to initially manifest tricuspid regurgitation and atrial flutter, and the 11th described patient with cardiac disease among infants exposed to lithium compounds in the first trimester of pregnancy. Sixty-three percent of these infants had tricuspid valve involvement. Lithium carbonate may be a factor in the increasing incidence of congenital heart disease when taken during early pregnancy. It also causes neurologic depression, cyanosis, and cardiac arrhythmia when consumed prior to delivery.</text>\n",
      "<annotation id=\"3\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D014262</infon>\n",
      "<location length=\"23\" offset=\"105\"></location>\n",
      "<text>tricuspid regurgitation</text>\n",
      "</annotation>\n",
      "<annotation id=\"4\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D001282</infon>\n",
      "<location length=\"14\" offset=\"130\"></location>\n",
      "<text>atrial flutter</text>\n",
      "</annotation>\n",
      "<annotation id=\"5\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D006333</infon>\n",
      "<location length=\"24\" offset=\"146\"></location>\n",
      "<text>congestive heart failure</text>\n",
      "</annotation>\n",
      "<annotation id=\"6\">\n",
      "<infon key=\"type\">Chemical</infon>\n",
      "<infon key=\"MESH\">D008094</infon>\n",
      "<location length=\"7\" offset=\"189\"></location>\n",
      "<text>lithium</text>\n",
      "</annotation>\n",
      "<annotation id=\"7\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D014262</infon>\n",
      "<location length=\"23\" offset=\"265\"></location>\n",
      "<text>tricuspid regurgitation</text>\n",
      "</annotation>\n",
      "<annotation id=\"8\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D001282</infon>\n",
      "<location length=\"14\" offset=\"293\"></location>\n",
      "<text>atrial flutter</text>\n",
      "</annotation>\n",
      "<annotation id=\"9\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D006331</infon>\n",
      "<location length=\"15\" offset=\"345\"></location>\n",
      "<text>cardiac disease</text>\n",
      "</annotation>\n",
      "<annotation id=\"10\">\n",
      "<infon key=\"type\">Chemical</infon>\n",
      "<infon key=\"MESH\">D008094</infon>\n",
      "<location length=\"7\" offset=\"386\"></location>\n",
      "<text>lithium</text>\n",
      "</annotation>\n",
      "<annotation id=\"11\">\n",
      "<infon key=\"type\">Chemical</infon>\n",
      "<infon key=\"MESH\">D016651</infon>\n",
      "<location length=\"17\" offset=\"511\"></location>\n",
      "<text>Lithium carbonate</text>\n",
      "</annotation>\n",
      "<annotation id=\"12\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D006331</infon>\n",
      "<location length=\"24\" offset=\"576\"></location>\n",
      "<text>congenital heart disease</text>\n",
      "</annotation>\n",
      "<annotation id=\"13\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D003866</infon>\n",
      "<location length=\"21\" offset=\"651\"></location>\n",
      "<text>neurologic depression</text>\n",
      "</annotation>\n",
      "<annotation id=\"14\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D003490</infon>\n",
      "<location length=\"8\" offset=\"674\"></location>\n",
      "<text>cyanosis</text>\n",
      "</annotation>\n",
      "<annotation id=\"15\">\n",
      "<infon key=\"type\">Disease</infon>\n",
      "<infon key=\"MESH\">D001145</infon>\n",
      "<location length=\"18\" offset=\"688\"></location>\n",
      "<text>cardiac arrhythmia</text>\n",
      "</annotation>\n",
      "</passage>\n",
      "--------------------\n",
      "A newborn with massive tricuspid regurgitation, atrial flutter, congestive heart failure, and a high serum lithium level is described. This is the first patient to initially manifest tricuspid regurgitation and atrial flutter, and the 11th described patient with cardiac disease among infants exposed to lithium compounds in the first trimester of pregnancy. Sixty-three percent of these infants had tricuspid valve involvement. Lithium carbonate may be a factor in the increasing incidence of congenital heart disease when taken during early pregnancy. It also causes neurologic depression, cyanosis, and cardiac arrhythmia when consumed prior to delivery.\n"
     ]
    }
   ],
   "source": [
    "def get_sample_text(passage_soup): \n",
    "    children_names = [child.name for child in passage_soup.children]\n",
    "    assert 'text' in children_names \n",
    "    sample_text = passage.find('text').text\n",
    "    return sample_text\n",
    "\n",
    "doc_id = first_document.find('id').text\n",
    "passages = first_document.find_all('passage')\n",
    "for passage_soup in passages:\n",
    "    print(passage_soup)\n",
    "    sample_text = get_sample_text(passage_soup)\n",
    "    print(\"-\"*20)\n",
    "    print(sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4a7dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n",
      "DROPBOX: connection verified\n"
     ]
    }
   ],
   "source": [
    "from utils.easy_testing import get_train_samples_by_dataset_name\n",
    "samples = get_train_samples_by_dataset_name('legaleval_preamble_vanilla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be599613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "from preamble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707009d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mhello\u001b[0m \u001b[32mhello\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(red('hello'), green('hello'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b9d44ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n",
      "truncating\n"
     ]
    }
   ],
   "source": [
    "print(len(samples))\n",
    "for sample in samples:\n",
    "    be_len = len(tokenizer(sample.text, truncation=True)['input_ids'])\n",
    "    be_tensor_len = tokenizer(sample.text, truncation=True, return_tensors='pt')['input_ids'].shape[1]\n",
    "    if be_len == tokenizer.model_max_length:\n",
    "        print(\"truncating\")\n",
    "    assert be_len == be_tensor_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c9943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelConfig(model_config_name='SpanWithSpanLengthRestriction', pretrained_model_name='xlm-roberta-large', pretrained_model_output_dim=1024, num_epochs=15, model_name='SpanNoTokenizationBatched', optimizer='Adam', learning_rate=1e-05, batch_size=4, max_span_length=30)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.config import get_model_config_from_path\n",
    "get_model_config_from_path('model_span_large_span_width_restriction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ec974f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "None\n",
      "2\n",
      "2\n",
      "None\n",
      "3\n",
      "None\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "sentence1 = 'this is a sentence'\n",
    "sentence2 = 'this is another sentence'\n",
    "be = tokenizer([sentence1, sentence2])\n",
    "for i in range(len(sentence1)):\n",
    "    print(be.char_to_token(batch_or_char_index=0, char_index=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d42b65d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tensor = torch.rand(3,3)\n",
    "tensor[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d07c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "be = tokenizer(['a sentence [SEP] another sentence', 'a sentence 2 [SEP] another sentecence 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7cc622c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 170, 5650, 102, 1330, 5650, 102], [101, 170, 5650, 123, 102, 1330, 1850, 10294, 7008, 123, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9d7795d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'a', 'sentence', '[SEP]', 'another', 'sentence', '[SEP]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1ab77c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'a', 'sentence', '2', '[SEP]', 'another', 'sent', '##ec', '##ence', '2', '[SEP]']\n",
      "a\n",
      "sentence\n",
      "2\n",
      "[SEP]\n",
      "another\n",
      "sent\n",
      "ec\n",
      "ence\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "sentence = 'a sentence 2 [SEP] another sentecence 2'\n",
    "tokens = be.tokens(batch_index=1)\n",
    "print(tokens)\n",
    "for i in range(len(tokens)):\n",
    "    char_span = be.token_to_chars(batch_or_token_index=1, token_index=i)\n",
    "    if char_span is not None:\n",
    "        print(sentence[char_span.start:char_span.end]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce77fc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(len('this is another sentence that is longer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "602d5118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 170, 5650, 102, 1330, 5650, 102],\n",
       " [101, 170, 5650, 123, 102, 1330, 1850, 10294, 7008, 123, 102]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e7f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.universal import Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9d6a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "069e8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee5beaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3670, 0.6071, 0.8106],\n",
       "         [0.4576, 0.3980, 0.9745]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7536393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "491878eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf2836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "every",
   "language": "python",
   "name": "every"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
